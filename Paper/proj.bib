
@article{wang_residual_2017,
	title = {Residual {Attention} {Network} for {Image} {Classification}},
	url = {http://arxiv.org/abs/1704.06904},
	abstract = {In this work, we propose "Residual Attention Network", a convolutional neural network using attention mechanism which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion. Our Residual Attention Network is built by stacking Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper. Inside each Attention Module, bottom-up top-down feedforward structure is used to unfold the feedforward and feedback attention process into a single feedforward process. Importantly, we propose attention residual learning to train very deep Residual Attention Networks which can be easily scaled up to hundreds of layers. Extensive analyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the effectiveness of every module mentioned above. Our Residual Attention Network achieves state-of-the-art object recognition performance on three benchmark datasets including CIFAR-10 (3.90\% error), CIFAR-100 (20.45\% error) and ImageNet (4.8\% single model and single crop, top-5 error). Note that, our method achieves 0.6\% top-1 accuracy improvement with 46\% trunk depth and 69\% forward FLOPs comparing to ResNet-200. The experiment also demonstrates that our network is robust against noisy labels.},
	urldate = {2022-05-08},
	journal = {arXiv:1704.06904 [cs]},
	author = {Wang, Fei and Jiang, Mengqing and Qian, Chen and Yang, Shuo and Li, Cheng and Zhang, Honggang and Wang, Xiaogang and Tang, Xiaoou},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.06904},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: accepted to CVPR2017},
	file = {arXiv Fulltext PDF:C\:\\Users\\giorg\\Zotero\\storage\\DUY9G28P\\Wang et al. - 2017 - Residual Attention Network for Image Classificatio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\giorg\\Zotero\\storage\\WB3BLZTG\\1704.html:text/html},
}

@article{sankar_regularizing_2017,
	title = {Regularizing deep networks using efficient layerwise adversarial training},
	abstract = {Adversarial training has been shown to regularize deep neural networks in addition to increasing their robustness to adversarial examples. However, its impact on very deep state of the art networks has not been fully investigated. In this paper, we present an efficient approach to perform adversarial training by perturbing intermediate layer activations and study the use of such perturbations as a regularizer during training. We use these perturbations to train very deep models such as ResNets and show improvement in performance both on adversarial and original test data. Our experiments highlight the benefits of perturbing intermediate layer activations compared to perturbing only the inputs. The results on CIFAR-10 and CIFAR-100 datasets show the merits of the proposed adversarial training approach. Additional results on WideResNets show that our approach provides significant improvement in classification accuracy for a given base model, outperforming dropout and other base models of larger size.},
	author = {Sankar, Swami and Jain, Arpit and Chellappa, Rama and Lim, Ser-Nam},
	month = may,
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\giorg\\Zotero\\storage\\D2MY462B\\Sankar et al. - 2017 - Regularizing deep networks using efficient layerwi.pdf:application/pdf},
}

@article{zeiler_visualizing_2013,
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1311.2901},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky {\textbackslash}etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	urldate = {2022-05-08},
	journal = {arXiv:1311.2901 [cs]},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	month = nov,
	year = {2013},
	note = {arXiv: 1311.2901
version: 3},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\giorg\\Zotero\\storage\\CNPMN9LL\\Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\giorg\\Zotero\\storage\\3HQ8DM93\\1311.html:text/html},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	urldate = {2022-05-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
	file = {Full Text PDF:C\:\\Users\\giorg\\Zotero\\storage\\5RG9F63D\\Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf},
}

@article{karthikeyan_review_2022,
	title = {Review of {Deep} {Transfer} {Learning} {Models} for {Image} {Classification}},
	volume = {10},
	copyright = {Copyright (c) 2022 Neelesh K},
	issn = {2197-8581},
	url = {https://online-journals.org/index.php/i-jes/article/view/29783},
	doi = {10.3991/ijes.v10i01.29783},
	abstract = {With the rapid rise in urbanization, solid waste generation has increased exceedingly. This study aims to develop a Convolutional Neural Network (CNN) to classify trash into biodegradable and non-biodegradable wastes. The TrashNet dataset was utilized in this study, and image augmentation was employed to make the model more robust against translation invariance. Transfer Learning methods based on CNN have shown promising outcomes on diverse image classification problems. This paper reviews the deep learning models available with pre-trained weights in the Keras library. The performance of the models was compared, and the model based on NASNetMobile had the highest accuracy of 97\%. Further, the model’s hyper-parameters were tuned, and the significance of a hyper-parameter on the model’s accuracy was studied.},
	language = {en},
	number = {01},
	urldate = {2022-05-08},
	journal = {International Journal of Recent Contributions from Engineering, Science \& IT (iJES)},
	author = {Karthikeyan, Neelesh},
	month = mar,
	year = {2022},
	note = {Number: 01},
	keywords = {TrashNet},
	pages = {17--28},
	file = {Full Text PDF:C\:\\Users\\giorg\\Zotero\\storage\\N836MAWZ\\Karthikeyan - 2022 - Review of Deep Transfer Learning Models for Image .pdf:application/pdf},
}

@misc{noauthor_chinese_nodate,
	title = {Chinese {MNIST}},
	url = {https://www.kaggle.com/gpreda/chinese-mnist},
	abstract = {Chinese numbers handwritten characters images},
	language = {en},
	urldate = {2022-05-08},
	journal = {Keggle},
	file = {Snapshot:C\:\\Users\\giorg\\Zotero\\storage\\DDRZPK98\\chinese-mnist.html:text/html},
}

@misc{noauthor_sign_nodate,
	title = {Sign {Language} {MNIST}},
	url = {https://www.kaggle.com/datamunge/sign-language-mnist},
	abstract = {Drop-In Replacement for MNIST for Hand Gesture Recognition Tasks},
	language = {en},
	urldate = {2022-05-08},
	journal = {Keggle},
	file = {Snapshot:C\:\\Users\\giorg\\Zotero\\storage\\7HJ33DW6\\sign-language-mnist.html:text/html},
}

@book{iba_deep_2020,
	title = {Deep {Neural} {Evolution}: {Deep} {Learning} with {Evolutionary} {Computation}},
	isbn = {9789811536854},
	shorttitle = {Deep {Neural} {Evolution}},
	abstract = {This book delivers the state of the art in deep learning (DL) methods hybridized with evolutionary computation (EC). Over the last decade, DL has dramatically reformed many domains: computer vision, speech recognition, healthcare, and automatic game playing, to mention only a few. All DL models, using different architectures and algorithms, utilize multiple processing layers for extracting a hierarchy of abstractions of data. Their remarkable successes notwithstanding, these powerful models are facing many challenges, and this book presents the collaborative efforts by researchers in EC to solve some of the problems in DL. EC comprises optimization techniques that are useful when problems are complex or poorly understood, or insufficient information about the problem domain is available. This family of algorithms has proven effective in solving problems with challenging characteristics such as non-convexity, non-linearity, noise, and irregularity, which dampen the performance of most classic optimization schemes. Furthermore, EC has been extensively and successfully applied in artificial neural network (ANN) research —from parameter estimation to structure optimization. Consequently, EC researchers are enthusiastic about applying their arsenal for the design and optimization of deep neural networks (DNN). This book brings together the recent progress in DL research where the focus is particularly on three sub-domains that integrate EC with DL: (1) EC for hyper-parameter optimization in DNN; (2) EC for DNN architecture design; and (3) Deep neuroevolution. The book also presents interesting applications of DL with EC in real-world problems, e.g., malware classification and object detection. Additionally, it covers recent applications of EC in DL, e.g. generative adversarial networks (GAN) training and adversarial attacks. The book aims to prompt and facilitate the research in DL with EC both in theory and in practice.},
	language = {en},
	publisher = {Springer Nature},
	author = {Iba, Hitoshi and Noman, Nasimul},
	month = may,
	year = {2020},
	note = {Google-Books-ID: jjPnDwAAQBAJ},
	keywords = {Computers / Artificial Intelligence / General, Computers / Information Technology, Mathematics / Applied},
}

@misc{lomonaco_core50_2022,
	title = {{CORe50}},
	url = {https://github.com/vlomonaco/core50},
	abstract = {CORe50: a new Dataset and Benchmark for Continual Learning},
	urldate = {2022-05-08},
	author = {Lomonaco, Vincenzo},
	month = apr,
	year = {2022},
	note = {original-date: 2017-04-25T17:56:29Z},
	keywords = {benchmark, caffe, computer-vision, continuous-learning, convolutional-neural-networks, dataset, deep-learning, object-recognition, paper},
}

@article{lin_clear_2021,
	title = {The {CLEAR} {Benchmark}: {Continual} {LEArning} on {Real}-{World} {Imagery}},
	abstract = {Continual learning (CL) is widely regarded as crucial challenge for lifelong AI. However, existing CL benchmarks, e.g. Permuted-MNIST and Split-CIFAR, make use of artiﬁcial temporal variation and do not align with or generalize to the realworld. In this paper, we introduce CLEAR, the ﬁrst continual image classiﬁcation benchmark dataset with a natural temporal evolution of visual concepts in the real world that spans a decade (2004-2014). We build CLEAR from existing large-scale image collections (YFCC100M) through a novel and scalable low-cost approach to visio-linguistic dataset curation. Our pipeline makes use of pretrained vision-language models (e.g. CLIP) to interactively build labeled datasets, which are further validated with crowd-sourcing to remove errors and even inappropriate images (hidden in original YFCC100M). The major strength of CLEAR over prior CL benchmarks is the smooth temporal evolution of visual concepts with real-world imagery, including both high-quality labeled data along with abundant unlabeled samples per time period for continual semi-supervised learning. We ﬁnd that a simple unsupervised pre-training step can already boost state-of-the-art CL algorithms that only utilize fully-supervised data. Our analysis also reveals that mainstream CL evaluation protocols that train and test on iid data artiﬁcially inﬂate performance of CL system. To address this, we propose novel "streaming" protocols for CL that always test on the (near) future. Interestingly, streaming protocols (a) can simplify dataset curation since today’s testset can be repurposed for tomorrow’s trainset and (b) can produce more generalizable models with more accurate estimates of performance since all labeled data from each time-period is used for both training and testing (unlike classic iid train-test splits).},
	language = {en},
	author = {Lin, Zhiqiu and Shi, Jia and Pathak, Deepak and Ramanan, Deva},
	year = {2021},
	pages = {13},
	file = {Lin et al. - The CLEAR Benchmark Continual LEArning on Real-Wo.pdf:C\:\\Users\\giorg\\Zotero\\storage\\YMEC7EKH\\Lin et al. - The CLEAR Benchmark Continual LEArning on Real-Wo.pdf:application/pdf},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2022-05-09},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
	file = {arXiv Fulltext PDF:C\:\\Users\\giorg\\Zotero\\storage\\9EK8MEAR\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\giorg\\Zotero\\storage\\7US7AXK7\\1512.html:text/html},
}

@misc{noauthor_caltech_nodate,
	title = {Caltech 256 {Image} {Dataset}},
	url = {https://www.kaggle.com/jessicali9530/caltech256},
	abstract = {Over 30,000 images in 256 object categories},
	language = {en},
	urldate = {2022-05-09},
	file = {Snapshot:C\:\\Users\\giorg\\Zotero\\storage\\L72DBAIH\\caltech256.html:text/html},
}

@misc{noauthor_cifar-100_nodate,
	title = {{CIFAR}-100 {Python}},
	url = {https://www.kaggle.com/fedesoriano/cifar100},
	abstract = {A collection of images commonly used to train computer vision algorithms},
	language = {en},
	urldate = {2022-05-09},
	file = {Snapshot:C\:\\Users\\giorg\\Zotero\\storage\\E3DVE8LC\\cifar100.html:text/html},
}

@misc{noauthor_fashion_nodate,
	title = {Fashion {MNIST}},
	url = {https://www.kaggle.com/zalando-research/fashionmnist},
	abstract = {An MNIST-like dataset of 70,000 28x28 labeled fashion images},
	language = {en},
	urldate = {2022-05-09},
	file = {Snapshot:C\:\\Users\\giorg\\Zotero\\storage\\YPXMP77A\\fashionmnist.html:text/html},
}

@misc{noauthor_cifar-10_nodate,
	title = {{CIFAR}-10 and {CIFAR}-100 datasets},
	url = {https://www.cs.toronto.edu/~kriz/cifar.html},
	urldate = {2022-05-09},
	file = {CIFAR-10 and CIFAR-100 datasets:C\:\\Users\\giorg\\Zotero\\storage\\2BW7FXD8\\cifar.html:text/html},
}

@misc{noauthor_papers_nodate,
	title = {Papers with {Code} - {MNIST} {Dataset}},
	url = {https://paperswithcode.com/dataset/mnist},
	abstract = {The MNIST database (Modified National Institute of Standards and Technology database) is a large collection of handwritten digits. It has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger NIST Special Database 3 (digits written by employees of the United States Census Bureau) and Special Database 1 (digits written by high school students) which contain monochrome images of handwritten digits. The digits have been size-normalized and centered in a fixed-size image. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.},
	language = {en},
	urldate = {2022-05-09},
	file = {Snapshot:C\:\\Users\\giorg\\Zotero\\storage\\XJRREB2G\\mnist.html:text/html},
}

@article{yellamraju_benchmarks_2018,
	title = {Benchmarks for {Image} {Classification} and {Other} {High}-dimensional {Pattern} {Recognition} {Problems}},
	url = {http://arxiv.org/abs/1806.05272},
	abstract = {A good classification method should yield more accurate results than simple heuristics. But there are classification problems, especially high-dimensional ones like the ones based on image/video data, for which simple heuristics can work quite accurately; the structure of the data in such problems is easy to uncover without any sophisticated or computationally expensive method. On the other hand, some problems have a structure that can only be found with sophisticated pattern recognition methods. We are interested in quantifying the difficulty of a given high-dimensional pattern recognition problem. We consider the case where the patterns come from two pre-determined classes and where the objects are represented by points in a high-dimensional vector space. However, the framework we propose is extendable to an arbitrarily large number of classes. We propose classification benchmarks based on simple random projection heuristics. Our benchmarks are 2D curves parameterized by the classification error and computational cost of these simple heuristics. Each curve divides the plane into a "positive- gain" and a "negative-gain" region. The latter contains methods that are ill-suited for the given classification problem. The former is divided into two by the curve asymptote; methods that lie in the small region under the curve but right of the asymptote merely provide a computational gain but no structural advantage over the random heuristics. We prove that the curve asymptotes are optimal (i.e. at Bayes error) in some cases, and thus no sophisticated method can provide a structural advantage over the random heuristics. Such classification problems, an example of which we present in our numerical experiments, provide poor ground for testing new pattern classification methods.},
	urldate = {2022-05-09},
	journal = {arXiv:1806.05272 [cs, stat]},
	author = {Yellamraju, Tarun and Hepp, Jonas and Boutin, Mireille},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.05272},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\giorg\\Zotero\\storage\\XUKS7ZB7\\Yellamraju et al. - 2018 - Benchmarks for Image Classification and Other High.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\giorg\\Zotero\\storage\\4GUDXJYG\\1806.html:text/html},
}

@article{caldeira_image_2020,
	title = {Image {Classification} {Benchmark} ({ICB})},
	volume = {142},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419307158},
	doi = {10.1016/j.eswa.2019.112998},
	abstract = {During any investigative work, it is necessary to confront our solutions to the already existing ones. This requires much work, namely to try to recreate the competing solutions and to use the same experimental conditions in order to obtain an objective comparison. Benchmarking is an activity of comparing performance metrics of systems in order to rank them or the activity of comparing a specific system to state-of-the-art alternatives. It would be convenient to have an image benchmarking ecosystem, not only to evaluate a personal solution but also to compare it with other previously proposed solutions. Upon designing a new or improved image classification pipeline, a convolution neural network or a specific algorithm modifying some detail, a user can add it to the benchmarking ecosystem and get a report on the performance of the solution, which can be compared to other solutions that were previously benchmarked under the same conditions. In this paper we describe the ICB - “Image Classification Benchmark” -, a prototype of a benchmarking ecosystem created to enable this vision. Besides describing how it works and how it was made flexible to incorporate any algorithm, we apply it to a specific comparison, as a proof-of-concept.},
	language = {en},
	urldate = {2022-05-09},
	journal = {Expert Systems with Applications},
	author = {Caldeira, Manuel and Martins, Pedro and Costa, Rogério Luís C. and Furtado, Pedro},
	month = mar,
	year = {2020},
	keywords = {Benchmark ecosystem, CNN, Food recognition, Image classification},
	pages = {112998},
	file = {ScienceDirect Snapshot:C\:\\Users\\giorg\\Zotero\\storage\\LG9GQ74Q\\S0957417419307158.html:text/html},
}
